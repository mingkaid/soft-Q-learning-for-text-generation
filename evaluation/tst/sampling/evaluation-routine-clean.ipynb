{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5efef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412468c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompted_gpt2 import PromptedGPT2Generator\n",
    "from evaluator import Evaluator\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b6c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_seed = 2\n",
    "default_lr = 1e-4\n",
    "default_top_k = 10\n",
    "default_sample_size = 32\n",
    "default_note = ''\n",
    "\n",
    "runs = [{'model': 'distilgpt2',\n",
    "         'machine': 'ec2-82',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '500-test',\n",
    "         'prompt': 'Errorbad WorseException BAD'},\n",
    "        \n",
    "        {'model': 'distilgpt2',\n",
    "         'machine': 'petuum-42',\n",
    "         'task': 'neg2pos',\n",
    "         'data': '500-test',\n",
    "         'prompt': ' RatingPros GOOD GOOD GOOD'},\n",
    "        \n",
    "        {'model': 'distilgpt2',\n",
    "         'machine': 'ec2-82',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '500-train-random',\n",
    "         'prompt': ' ErrorsError Parameters BADBad'},\n",
    "        \n",
    "        {'model': 'distilgpt2',\n",
    "         'machine': 'ec2-82',\n",
    "         'task': 'neg2pos',\n",
    "         'data': '500-train-random',\n",
    "         'prompt': ' Description575 praises Excellent GREAT'},\n",
    "        \n",
    "        {'model': 'distilgpt2',\n",
    "         'machine': 'petuum-203',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '100-train-first',\n",
    "         'prompt': ' problemErrorBadExceptionBad'},\n",
    "        \n",
    "        {'model': 'distilgpt2',\n",
    "         'machine': 'petuum-203',\n",
    "         'task': 'neg2pos',\n",
    "         'data': '100-train-first',\n",
    "         'prompt': ' mediumExcellentExcellent GREAT GREAT'},\n",
    "        \n",
    "        {'model': 'gpt2',\n",
    "         'machine': 'petuum-203',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '500-test',\n",
    "         'prompt': 'Contents ERROR Values ERROR Values'},\n",
    "        \n",
    "        {'model': 'gpt2',\n",
    "         'machine': 'petuum-203',\n",
    "         'task': 'neg2pos',\n",
    "         'data': '500-test',\n",
    "         'prompt': ' attributes happiest Parameters Happiness=['},\n",
    "        \n",
    "        {'model': 'gpt2-medium',\n",
    "         'machine': 'petuum-42',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '500-test',\n",
    "         'prompt': 'icultyException ConditionException Either'},\n",
    "        \n",
    "        {'model': 'gpt2-medium',\n",
    "         'machine': 'ec2-82',\n",
    "         'task': 'neg2pos',\n",
    "         'data': '500-test',\n",
    "         'prompt': ' value MeaningHappy positives (%'},\n",
    "        \n",
    "        {'model': 'gpt2-large',\n",
    "         'machine': 'ec2-82',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '500-test',\n",
    "         'prompt': 'Problem objection discrepancyDERR contrasts'},\n",
    "        \n",
    "        {'model': 'gpt2-large',\n",
    "         'machine': 'ec2-94',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '500-test',\n",
    "         'prompt': 'AvailabilityDisable Suppose contradictory probabilities'},\n",
    "        \n",
    "        {'model': 'gpt2-large',\n",
    "         'machine': 'ec2-82',\n",
    "         'task': 'neg2pos',\n",
    "         'data': '500-test',\n",
    "         'lr': 5e-5,\n",
    "         'prompt': 'White happiest preferences (− happy'},\n",
    "        \n",
    "        {'model': 'gpt2-xl',\n",
    "         'machine': 'ec2-82',\n",
    "         'task': 'pos2neg',\n",
    "         'data': '500-test',\n",
    "         'prompt': 'Error [-either [-Neither'},\n",
    "       ]\n",
    "        \n",
    "new_runs = []\n",
    "for r in runs: \n",
    "    if 'seed' not in r: r['seed'] = default_seed\n",
    "    if 'lr' not in r: r['lr'] = default_lr\n",
    "    if 'top_k' not in r: r['top_k'] = default_top_k\n",
    "    if 'sample_size' not in r: r['sample_size'] = default_sample_size\n",
    "    if 'note' not in r: r['note'] = default_note\n",
    "    new_runs.append(r)\n",
    "runs = new_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a714157f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0396ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs(run, \n",
    "                     device=None, \n",
    "                     reward_device=None, \n",
    "                     generator_device=None,\n",
    "                     evaluator_device=None,\n",
    "                     raw_save_path='./raw',\n",
    "                     summary_save_path='./summary'): \n",
    "    if device is not None: \n",
    "        reward_device=device\n",
    "        generator_device=device\n",
    "        evaluator_device=device\n",
    "        \n",
    "    model = run['model']\n",
    "    task = run['task']\n",
    "    sample_size = run['sample_size']\n",
    "    top_k = run['top_k']\n",
    "    \n",
    "    dummy_prompts = {'pos2neg': '', 'neg2pos': ''}\n",
    "    generator = PromptedGPT2Generator(model, \n",
    "                                      dummy_prompts,\n",
    "                                      reward_device=reward_device,\n",
    "                                      generator_device=generator_device)\n",
    "    evaluator = Evaluator(evaluator_device)\n",
    "    \n",
    "    start = time.time()\n",
    "    output_list = generator.sample_generate(task, \n",
    "                                            sample_size, \n",
    "                                            top_k=top_k, \n",
    "                                            top_p=None, \n",
    "                                            single_prompt=run['prompt'])\n",
    "    time_elapsed = time.time() - start\n",
    "    del generator\n",
    "    \n",
    "    summary, output_df = evaluator.evaluate_output(task, \n",
    "                                                   output_list)\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    del evaluator\n",
    "    \n",
    "    summary.update({'model': model,\n",
    "                    'task': task,\n",
    "                    'sample_size': sample_size,\n",
    "                    'top_k': top_k,\n",
    "                    'lr': run['lr'],\n",
    "                    'seed': run['seed'],\n",
    "                    'machine': run['machine'],\n",
    "                    'data': run['data'],\n",
    "                    'time_elapsed': round(time_elapsed, 2),\n",
    "                    'timestamp': timestamp,\n",
    "                    'note': run['note']})\n",
    "    print(summary)\n",
    "    \n",
    "    output_name = f\"{task}_{model}_{run['data']}_{run['machine']}_{timestamp}\"\n",
    "    json.dump(summary, open(os.path.join(summary_save_path, output_name + '.json'), 'w'))\n",
    "    output_df.to_csv(os.path.join(raw_save_path, output_name + '.csv'), index=False)\n",
    "    \n",
    "    return summary, output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3913791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  2%|▏         | 10/500 [00:15<11:05,  1.36s/it]/opt/conda/envs/sql-203/lib/python3.8/site-packages/transformers/pipelines/base.py:997: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 500/500 [11:28<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing with reference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 4799.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 105.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 12851.46it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 94.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sum_reward': 81.6, 'recon': 65.09, 'self_bleu': 38.83, 'ref_bleu': 21.45, 'style_acc': 0.95, 'ppl': 46.77, 'model': 'gpt2-large', 'task': 'neg2pos', 'sample_size': 32, 'top_k': 10, 'lr': 0.0001, 'seed': 2, 'machine': 'ec2-82', 'data': '500-test', 'time_elapsed': 688.97, 'timestamp': '2022-04-26_14:12:34', 'note': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run = runs[-2]\n",
    "summary, output_df = generate_outputs(run, reward_device=3, generator_device=2, evaluator_device=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373ad82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "10it [00:04,  2.18it/s]/opt/conda/envs/sql-203/lib/python3.8/site-packages/transformers/pipelines/base.py:997: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "500it [03:48,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing with reference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 5388.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 128.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 13777.84it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 100.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sum_reward': 75.34, 'recon': 52.16, 'self_bleu': 22.89, 'ref_bleu': 14, 'style_acc': 0.99, 'ppl': 34.68, 'model': 'distilgpt2', 'task': 'neg2pos', 'sample_size': 32, 'top_k': 10, 'lr': 0.0001, 'seed': 2, 'machine': 'petuum-42', 'data': '500-test', 'time_elapsed': 228.76, 'timestamp': '2022-04-25_22:24:27', 'note': ''}\n"
     ]
    }
   ],
   "source": [
    "summaries, output_dfs = [], []\n",
    "for r in runs: \n",
    "    summary, output_df = generate_outputs(r, \n",
    "                                          reward_device=3, \n",
    "                                          generator_device=2, \n",
    "                                          evaluator_device=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a649460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:13, 38.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "neg2pos_ref_output = []\n",
    "for i, (src, ref) in tqdm(enumerate(zip(generator.sentence_dict['src_neg2pos'],\n",
    "                                        evaluator.sentence_dict['ref_neg2pos']))): \n",
    "    output = generator._select_output([ref],\n",
    "                                       [src], \n",
    "                                       'LABEL_1', \n",
    "                                       sample_id=i)\n",
    "    neg2pos_ref_output.append(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql-203",
   "language": "python",
   "name": "sql-203"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
