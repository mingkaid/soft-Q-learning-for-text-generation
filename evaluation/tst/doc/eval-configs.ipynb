{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6867d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012d3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff0eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./experiment-eval-configs-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487a3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[((df.Machine.str.contains('ec2')) | (df.Model == 'gpt2-large'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4aed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query('Status == \"DONE\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d46405",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/jupyter/prompt-generation/soft-Q-learning-for-text-generation/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a095621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['checkpoint_exists'] = df['Checkpoint Path'].apply(lambda x: os.path.exists(os.path.join(output_path, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24628cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Size</th>\n",
       "      <th>Task</th>\n",
       "      <th>Machine</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Checkpoint Path</th>\n",
       "      <th>Status</th>\n",
       "      <th>Note</th>\n",
       "      <th>checkpoint_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-18</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-02/23-32-32/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-34</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-02/23-19-02/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-08/01-50-29/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-02/22-39-22/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-203</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-05-09/02-56-07/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-29/14-26-13/outputs/outputs.94.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-18</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-07/19-26-36/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-230</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-09/02-46-03/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-94</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-02/22-51-41/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>petuum-42</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-02/22-48-37/outputs/outputs.226.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>petuum-203</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-02/22-47-18/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-94</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-07/19-16-07/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-09/16-28-03/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-94</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-09/17-51-42/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-10/14-31-13/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-04/20-35-39/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-04/20-36-23/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-07/19-32-23/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-07/19-32-31/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-10/14-31-34/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-17/16-19-34/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-230</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-17/16-27-26/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-17/16-23-03/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-17/16-24-27/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>16</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-34</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-07/19-21-41/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 0-15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>16</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-09/15-52-51/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 32-47</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>16</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-84</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-09/02-51-03/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 16-31</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>16</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-228</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-07/19-23-59/outputs/outputs.58.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 0-15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>16</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-09/15-52-10/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 32-47</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>16</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-54</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-11/13-58-25/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 0-15, lr=1e-4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>16</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-54</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-09/02-53-15/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 16-31</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1k</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-12/16-07-49/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 2K-3K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1k</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-18</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-10/14-22-43/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 1K-2K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1k</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-05/01-22-49/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 0-1K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1k</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-05-05/01-47-28/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 0-1K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1k</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-228</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-12/16-01-06/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 0-1K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1k</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-34</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-12/16-03-17/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 1K-2K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1k</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-12/16-16-55/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 2K-3K</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>100</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-230</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-11/13-56-54/outputs/outputs.234.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>examples 0-99</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Data Size     Task     Machine  Seed  \\\n",
       "0       gpt2-xl       all  neg2pos      ec2-18     2   \n",
       "1       gpt2-xl       all  neg2pos      ec2-34     3   \n",
       "2       gpt2-xl       all  neg2pos      ec2-82     3   \n",
       "3       gpt2-xl       all  neg2pos      ec2-82     4   \n",
       "4       gpt2-xl       all  neg2pos     ec2-203     5   \n",
       "5       gpt2-xl       all  pos2neg      ec2-82     2   \n",
       "6       gpt2-xl       all  pos2neg      ec2-18     3   \n",
       "7       gpt2-xl       all  pos2neg     ec2-230     4   \n",
       "8    gpt2-large       all  neg2pos      ec2-94     3   \n",
       "9    gpt2-large       all  neg2pos   petuum-42     3   \n",
       "10   gpt2-large       all  neg2pos  petuum-203     2   \n",
       "11   gpt2-large       all  pos2neg      ec2-94     3   \n",
       "12   gpt2-large       all  pos2neg      ec2-82     2   \n",
       "13   gpt2-large       all  pos2neg      ec2-94     4   \n",
       "14  gpt2-medium       all  neg2pos     ec2-239     2   \n",
       "15  gpt2-medium       all  neg2pos      ec2-82     3   \n",
       "16  gpt2-medium       all  neg2pos      ec2-82     4   \n",
       "17  gpt2-medium       all  pos2neg     ec2-239     2   \n",
       "18  gpt2-medium       all  pos2neg     ec2-239     3   \n",
       "19  gpt2-medium       all  pos2neg     ec2-239     4   \n",
       "32      gpt2-xl       all  pos2neg      ec2-82     2   \n",
       "33      gpt2-xl       all  pos2neg     ec2-230     3   \n",
       "34      gpt2-xl       all  neg2pos      ec2-82     2   \n",
       "35      gpt2-xl       all  neg2pos      ec2-82     3   \n",
       "36      gpt2-xl        16  neg2pos      ec2-34     2   \n",
       "37      gpt2-xl        16  neg2pos      ec2-82     3   \n",
       "38      gpt2-xl        16  neg2pos      ec2-84     4   \n",
       "39      gpt2-xl        16  pos2neg     ec2-228     2   \n",
       "40      gpt2-xl        16  pos2neg      ec2-82     3   \n",
       "41      gpt2-xl        16  neg2pos      ec2-54     2   \n",
       "44      gpt2-xl        16  pos2neg      ec2-54     4   \n",
       "45      gpt2-xl        1k  neg2pos      ec2-82     2   \n",
       "46      gpt2-xl        1k  neg2pos      ec2-18     3   \n",
       "47      gpt2-xl        1k  neg2pos      ec2-82     4   \n",
       "48      gpt2-xl        1k  neg2pos      ec2-82     5   \n",
       "49      gpt2-xl        1k  pos2neg     ec2-228     2   \n",
       "50      gpt2-xl        1k  pos2neg      ec2-34     3   \n",
       "51      gpt2-xl        1k  pos2neg      ec2-82     4   \n",
       "52      gpt2-xl       100  neg2pos     ec2-230     2   \n",
       "\n",
       "                                Checkpoint Path Status  \\\n",
       "0   2022-05-02/23-32-32/outputs/outputs.232.pth   DONE   \n",
       "1   2022-05-02/23-19-02/outputs/outputs.232.pth   DONE   \n",
       "2   2022-05-08/01-50-29/outputs/outputs.232.pth   DONE   \n",
       "3   2022-05-02/22-39-22/outputs/outputs.232.pth   DONE   \n",
       "4   2022-05-09/02-56-07/outputs/outputs.232.pth   DONE   \n",
       "5    2022-04-29/14-26-13/outputs/outputs.94.pth   DONE   \n",
       "6   2022-05-07/19-26-36/outputs/outputs.116.pth   DONE   \n",
       "7   2022-05-09/02-46-03/outputs/outputs.116.pth   DONE   \n",
       "8   2022-05-02/22-51-41/outputs/outputs.232.pth   DONE   \n",
       "9   2022-05-02/22-48-37/outputs/outputs.226.pth   DONE   \n",
       "10  2022-05-02/22-47-18/outputs/outputs.232.pth   DONE   \n",
       "11  2022-05-07/19-16-07/outputs/outputs.116.pth   DONE   \n",
       "12  2022-05-09/16-28-03/outputs/outputs.116.pth   DONE   \n",
       "13  2022-05-09/17-51-42/outputs/outputs.116.pth   DONE   \n",
       "14  2022-05-10/14-31-13/outputs/outputs.232.pth   DONE   \n",
       "15  2022-05-04/20-35-39/outputs/outputs.232.pth   DONE   \n",
       "16  2022-05-04/20-36-23/outputs/outputs.232.pth   DONE   \n",
       "17  2022-05-07/19-32-23/outputs/outputs.116.pth   DONE   \n",
       "18  2022-05-07/19-32-31/outputs/outputs.116.pth   DONE   \n",
       "19  2022-05-10/14-31-34/outputs/outputs.116.pth   DONE   \n",
       "32  2022-05-17/16-19-34/outputs/outputs.116.pth   DONE   \n",
       "33  2022-05-17/16-27-26/outputs/outputs.116.pth   DONE   \n",
       "34  2022-05-17/16-23-03/outputs/outputs.116.pth   DONE   \n",
       "35  2022-05-17/16-24-27/outputs/outputs.116.pth   DONE   \n",
       "36  2022-05-07/19-21-41/outputs/outputs.232.pth   DONE   \n",
       "37  2022-05-09/15-52-51/outputs/outputs.232.pth   DONE   \n",
       "38  2022-05-09/02-51-03/outputs/outputs.232.pth   DONE   \n",
       "39   2022-05-07/19-23-59/outputs/outputs.58.pth   DONE   \n",
       "40  2022-05-09/15-52-10/outputs/outputs.116.pth   DONE   \n",
       "41  2022-05-11/13-58-25/outputs/outputs.232.pth   DONE   \n",
       "44  2022-05-09/02-53-15/outputs/outputs.116.pth   DONE   \n",
       "45  2022-05-12/16-07-49/outputs/outputs.232.pth   DONE   \n",
       "46  2022-05-10/14-22-43/outputs/outputs.232.pth   DONE   \n",
       "47  2022-05-05/01-22-49/outputs/outputs.232.pth   DONE   \n",
       "48  2022-05-05/01-47-28/outputs/outputs.232.pth   DONE   \n",
       "49  2022-05-12/16-01-06/outputs/outputs.116.pth   DONE   \n",
       "50  2022-05-12/16-03-17/outputs/outputs.116.pth   DONE   \n",
       "51  2022-05-12/16-16-55/outputs/outputs.116.pth   DONE   \n",
       "52  2022-05-11/13-56-54/outputs/outputs.234.pth   DONE   \n",
       "\n",
       "                      Note  checkpoint_exists  \n",
       "0                      NaN               True  \n",
       "1                      NaN               True  \n",
       "2                      NaN               True  \n",
       "3                      NaN               True  \n",
       "4                      NaN               True  \n",
       "5                      NaN               True  \n",
       "6                      NaN               True  \n",
       "7                      NaN               True  \n",
       "8                      NaN               True  \n",
       "9                      NaN               True  \n",
       "10                     NaN               True  \n",
       "11                     NaN               True  \n",
       "12                     NaN               True  \n",
       "13                     NaN               True  \n",
       "14                     NaN               True  \n",
       "15                     NaN               True  \n",
       "16                     NaN               True  \n",
       "17                     NaN               True  \n",
       "18                     NaN               True  \n",
       "19                     NaN               True  \n",
       "32                  fluent               True  \n",
       "33                  fluent               True  \n",
       "34                  fluent               True  \n",
       "35                  fluent               True  \n",
       "36           examples 0-15               True  \n",
       "37          examples 32-47               True  \n",
       "38          examples 16-31               True  \n",
       "39           examples 0-15               True  \n",
       "40          examples 32-47               True  \n",
       "41  examples 0-15, lr=1e-4               True  \n",
       "44          examples 16-31               True  \n",
       "45          examples 2K-3K               True  \n",
       "46          examples 1K-2K               True  \n",
       "47           examples 0-1K               True  \n",
       "48           examples 0-1K               True  \n",
       "49           examples 0-1K               True  \n",
       "50          examples 1K-2K               True  \n",
       "51          examples 2K-3K               True  \n",
       "52           examples 0-99               True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc23a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = (df['Model'] + '_' + \n",
    "                  df['Data Size'] + '_' + \n",
    "                  df['Task'] + '_' + \n",
    "                  df['Machine'] + '_' + \n",
    "                  df['Seed'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45ff66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Data Size'] == 'all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e171460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file2drop = ['gpt2-xl_all_neg2pos_ec2-18_2',\n",
    "#              'gpt2-xl_all_neg2pos_ec2-34_3',\n",
    "#              'gpt2-xl_all_neg2pos_ec2-82_4']\n",
    "file2drop = ['gpt2-xl_all_neg2pos_ec2-82_3',\n",
    "             'gpt2-xl_all_neg2pos_ec2-203_5']\n",
    "df = df[(~df['filename'].isin(file2drop)) | (df['Note'] == 'fluent')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82670ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Note.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f184ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['idx'] = df.groupby(['Model', 'Data Size', 'Task', 'Note']).cumcount()\n",
    "df.loc[(df['idx'] == 2), 'idx'] = 3\n",
    "df.loc[(df['idx'] == 1), 'idx'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d5f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename_idx'] = (df['Model'] + '_' + \n",
    "                      df['Data Size'] + '_' + \n",
    "                      df['Task'] + '_' + \n",
    "                      df['Machine'] + '_' + \n",
    "                      df['Seed'].astype(str) + '_' + \n",
    "                      df['idx'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bbd91fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Size</th>\n",
       "      <th>Task</th>\n",
       "      <th>Machine</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Checkpoint Path</th>\n",
       "      <th>Status</th>\n",
       "      <th>Note</th>\n",
       "      <th>checkpoint_exists</th>\n",
       "      <th>filename</th>\n",
       "      <th>idx</th>\n",
       "      <th>filename_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-18</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-02/23-32-32/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-18_2</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-18_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-34</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-02/23-19-02/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-34_3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-34_3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-02/22-39-22/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-82_4</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-82_4_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-04-29/14-26-13/outputs/outputs.94.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-82_2</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-82_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-18</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-07/19-26-36/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-18_3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-18_3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-230</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-09/02-46-03/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-230_4</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-230_4_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-94</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-02/22-51-41/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-large_all_neg2pos_ec2-94_3</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-large_all_neg2pos_ec2-94_3_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>petuum-42</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-02/22-48-37/outputs/outputs.226.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-large_all_neg2pos_petuum-42_3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-large_all_neg2pos_petuum-42_3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>petuum-203</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-02/22-47-18/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-large_all_neg2pos_petuum-203_2</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt2-large_all_neg2pos_petuum-203_2_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-94</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-07/19-16-07/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-large_all_pos2neg_ec2-94_3</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-large_all_pos2neg_ec2-94_3_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-09/16-28-03/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-large_all_pos2neg_ec2-82_2</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-large_all_pos2neg_ec2-82_2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-94</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-09/17-51-42/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-large_all_pos2neg_ec2-94_4</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt2-large_all_pos2neg_ec2-94_4_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-10/14-31-13/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-medium_all_neg2pos_ec2-239_2</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-medium_all_neg2pos_ec2-239_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-04/20-35-39/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-medium_all_neg2pos_ec2-82_3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-medium_all_neg2pos_ec2-82_3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-04/20-36-23/outputs/outputs.232.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-medium_all_neg2pos_ec2-82_4</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt2-medium_all_neg2pos_ec2-82_4_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-07/19-32-23/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-medium_all_pos2neg_ec2-239_2</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-medium_all_pos2neg_ec2-239_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-07/19-32-31/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-medium_all_pos2neg_ec2-239_3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-medium_all_pos2neg_ec2-239_3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-239</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-10/14-31-34/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-medium_all_pos2neg_ec2-239_4</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt2-medium_all_pos2neg_ec2-239_4_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-17/16-19-34/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-82_2</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-82_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>pos2neg</td>\n",
       "      <td>ec2-230</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-17/16-27-26/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-230_3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-xl_all_pos2neg_ec2-230_3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-05-17/16-23-03/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-82_2</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-82_2_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>neg2pos</td>\n",
       "      <td>ec2-82</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-05-17/16-24-27/outputs/outputs.116.pth</td>\n",
       "      <td>DONE</td>\n",
       "      <td>fluent</td>\n",
       "      <td>True</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-82_3</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt2-xl_all_neg2pos_ec2-82_3_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Data Size     Task     Machine  Seed  \\\n",
       "0       gpt2-xl       all  neg2pos      ec2-18     2   \n",
       "1       gpt2-xl       all  neg2pos      ec2-34     3   \n",
       "3       gpt2-xl       all  neg2pos      ec2-82     4   \n",
       "5       gpt2-xl       all  pos2neg      ec2-82     2   \n",
       "6       gpt2-xl       all  pos2neg      ec2-18     3   \n",
       "7       gpt2-xl       all  pos2neg     ec2-230     4   \n",
       "8    gpt2-large       all  neg2pos      ec2-94     3   \n",
       "9    gpt2-large       all  neg2pos   petuum-42     3   \n",
       "10   gpt2-large       all  neg2pos  petuum-203     2   \n",
       "11   gpt2-large       all  pos2neg      ec2-94     3   \n",
       "12   gpt2-large       all  pos2neg      ec2-82     2   \n",
       "13   gpt2-large       all  pos2neg      ec2-94     4   \n",
       "14  gpt2-medium       all  neg2pos     ec2-239     2   \n",
       "15  gpt2-medium       all  neg2pos      ec2-82     3   \n",
       "16  gpt2-medium       all  neg2pos      ec2-82     4   \n",
       "17  gpt2-medium       all  pos2neg     ec2-239     2   \n",
       "18  gpt2-medium       all  pos2neg     ec2-239     3   \n",
       "19  gpt2-medium       all  pos2neg     ec2-239     4   \n",
       "32      gpt2-xl       all  pos2neg      ec2-82     2   \n",
       "33      gpt2-xl       all  pos2neg     ec2-230     3   \n",
       "34      gpt2-xl       all  neg2pos      ec2-82     2   \n",
       "35      gpt2-xl       all  neg2pos      ec2-82     3   \n",
       "\n",
       "                                Checkpoint Path Status    Note  \\\n",
       "0   2022-05-02/23-32-32/outputs/outputs.232.pth   DONE           \n",
       "1   2022-05-02/23-19-02/outputs/outputs.232.pth   DONE           \n",
       "3   2022-05-02/22-39-22/outputs/outputs.232.pth   DONE           \n",
       "5    2022-04-29/14-26-13/outputs/outputs.94.pth   DONE           \n",
       "6   2022-05-07/19-26-36/outputs/outputs.116.pth   DONE           \n",
       "7   2022-05-09/02-46-03/outputs/outputs.116.pth   DONE           \n",
       "8   2022-05-02/22-51-41/outputs/outputs.232.pth   DONE           \n",
       "9   2022-05-02/22-48-37/outputs/outputs.226.pth   DONE           \n",
       "10  2022-05-02/22-47-18/outputs/outputs.232.pth   DONE           \n",
       "11  2022-05-07/19-16-07/outputs/outputs.116.pth   DONE           \n",
       "12  2022-05-09/16-28-03/outputs/outputs.116.pth   DONE           \n",
       "13  2022-05-09/17-51-42/outputs/outputs.116.pth   DONE           \n",
       "14  2022-05-10/14-31-13/outputs/outputs.232.pth   DONE           \n",
       "15  2022-05-04/20-35-39/outputs/outputs.232.pth   DONE           \n",
       "16  2022-05-04/20-36-23/outputs/outputs.232.pth   DONE           \n",
       "17  2022-05-07/19-32-23/outputs/outputs.116.pth   DONE           \n",
       "18  2022-05-07/19-32-31/outputs/outputs.116.pth   DONE           \n",
       "19  2022-05-10/14-31-34/outputs/outputs.116.pth   DONE           \n",
       "32  2022-05-17/16-19-34/outputs/outputs.116.pth   DONE  fluent   \n",
       "33  2022-05-17/16-27-26/outputs/outputs.116.pth   DONE  fluent   \n",
       "34  2022-05-17/16-23-03/outputs/outputs.116.pth   DONE  fluent   \n",
       "35  2022-05-17/16-24-27/outputs/outputs.116.pth   DONE  fluent   \n",
       "\n",
       "    checkpoint_exists                             filename  idx  \\\n",
       "0                True         gpt2-xl_all_neg2pos_ec2-18_2    0   \n",
       "1                True         gpt2-xl_all_neg2pos_ec2-34_3    2   \n",
       "3                True         gpt2-xl_all_neg2pos_ec2-82_4    3   \n",
       "5                True         gpt2-xl_all_pos2neg_ec2-82_2    0   \n",
       "6                True         gpt2-xl_all_pos2neg_ec2-18_3    2   \n",
       "7                True        gpt2-xl_all_pos2neg_ec2-230_4    3   \n",
       "8                True      gpt2-large_all_neg2pos_ec2-94_3    0   \n",
       "9                True   gpt2-large_all_neg2pos_petuum-42_3    2   \n",
       "10               True  gpt2-large_all_neg2pos_petuum-203_2    3   \n",
       "11               True      gpt2-large_all_pos2neg_ec2-94_3    0   \n",
       "12               True      gpt2-large_all_pos2neg_ec2-82_2    2   \n",
       "13               True      gpt2-large_all_pos2neg_ec2-94_4    3   \n",
       "14               True    gpt2-medium_all_neg2pos_ec2-239_2    0   \n",
       "15               True     gpt2-medium_all_neg2pos_ec2-82_3    2   \n",
       "16               True     gpt2-medium_all_neg2pos_ec2-82_4    3   \n",
       "17               True    gpt2-medium_all_pos2neg_ec2-239_2    0   \n",
       "18               True    gpt2-medium_all_pos2neg_ec2-239_3    2   \n",
       "19               True    gpt2-medium_all_pos2neg_ec2-239_4    3   \n",
       "32               True         gpt2-xl_all_pos2neg_ec2-82_2    0   \n",
       "33               True        gpt2-xl_all_pos2neg_ec2-230_3    2   \n",
       "34               True         gpt2-xl_all_neg2pos_ec2-82_2    0   \n",
       "35               True         gpt2-xl_all_neg2pos_ec2-82_3    2   \n",
       "\n",
       "                             filename_idx  \n",
       "0          gpt2-xl_all_neg2pos_ec2-18_2_0  \n",
       "1          gpt2-xl_all_neg2pos_ec2-34_3_2  \n",
       "3          gpt2-xl_all_neg2pos_ec2-82_4_3  \n",
       "5          gpt2-xl_all_pos2neg_ec2-82_2_0  \n",
       "6          gpt2-xl_all_pos2neg_ec2-18_3_2  \n",
       "7         gpt2-xl_all_pos2neg_ec2-230_4_3  \n",
       "8       gpt2-large_all_neg2pos_ec2-94_3_0  \n",
       "9    gpt2-large_all_neg2pos_petuum-42_3_2  \n",
       "10  gpt2-large_all_neg2pos_petuum-203_2_3  \n",
       "11      gpt2-large_all_pos2neg_ec2-94_3_0  \n",
       "12      gpt2-large_all_pos2neg_ec2-82_2_2  \n",
       "13      gpt2-large_all_pos2neg_ec2-94_4_3  \n",
       "14    gpt2-medium_all_neg2pos_ec2-239_2_0  \n",
       "15     gpt2-medium_all_neg2pos_ec2-82_3_2  \n",
       "16     gpt2-medium_all_neg2pos_ec2-82_4_3  \n",
       "17    gpt2-medium_all_pos2neg_ec2-239_2_0  \n",
       "18    gpt2-medium_all_pos2neg_ec2-239_3_2  \n",
       "19    gpt2-medium_all_pos2neg_ec2-239_4_3  \n",
       "32         gpt2-xl_all_pos2neg_ec2-82_2_0  \n",
       "33        gpt2-xl_all_pos2neg_ec2-230_3_2  \n",
       "34         gpt2-xl_all_neg2pos_ec2-82_2_0  \n",
       "35         gpt2-xl_all_neg2pos_ec2-82_3_2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0759f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt_template = ('CUDA_VISIBLE_DEVICES={device_id} python generate_prompts.py --task={task} --ckpt_path_short={ckpt_path_short} '\n",
    "                            '--device_id=0 --output_path=./outputs/prompts/{filename}.out')\n",
    "\n",
    "prompted_gpt2_template = ('python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/{filename}.out '\n",
    "                          '--src_path=./data/yelp/test.clean.{src_label} --save_path=./outputs/hypos/{filename}.jsonl '\n",
    "                          '--device_id={device_id} --model_name={model_name}')\n",
    "\n",
    "yelp_select_output_template = ('python yelp_output_selector.py select-and-save --device_id={device_id} '\n",
    "                               '--pos2neg_hypos_path=./outputs/hypos/{pos2neg_filename}.jsonl '\n",
    "                               '--neg2pos_hypos_path=./outputs/hypos/{neg2pos_filename}.jsonl '\n",
    "                               '--save_path=./outputs/selected/{combined_filename}.jsonl')\n",
    "\n",
    "yelp_evaluate_template = ('python yelp_evaluator.py evaluate-and-save --device_id={device_id} '\n",
    "                          '--hypos_path=./outputs/selected/{combined_filename}.jsonl '\n",
    "                          '--pos2neg_refs_path=./data/yelp/ref.clean.1 '\n",
    "                          '--neg2pos_refs_path=./data/yelp/ref.clean.0 '\n",
    "                          '--summary_save_path=./outputs/eval_summaries/{combined_filename}.json '\n",
    "                          '--results_save_path=./outputs/eval_results/{combined_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "13080896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_commands(subdf, n_trials=5, device_id=None): \n",
    "    assert subdf.shape[0] == 2\n",
    "    # print(subdf.columns)\n",
    "    # print(subdf)\n",
    "    all_commands = []\n",
    "    for i in range(5, 9):\n",
    "        generate_prompt_commands = (subdf.apply(\n",
    "            lambda row: generate_prompt_template.format(task=row['Task'],\n",
    "                                                        ckpt_path_short=row['Checkpoint Path'],\n",
    "                                                        device_id=device_id,\n",
    "                                                        filename='_'.join([row['filename_idx'], str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        prompted_gpt2_commands = (subdf.apply(\n",
    "            lambda row: prompted_gpt2_template.format(src_label=str(1 if row['Task'] == 'pos2neg' else 0),\n",
    "                                                      model_name=row['Model'],\n",
    "                                                      device_id=device_id,\n",
    "                                                      filename='_'.join([row['filename_idx'], str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        pos2neg_filename = '_'.join([subdf.query('Task == \"pos2neg\"').iloc[0].filename_idx, str(i)])\n",
    "        neg2pos_filename = '_'.join([subdf.query('Task == \"neg2pos\"').iloc[0].filename_idx, str(i)])\n",
    "        row = subdf.iloc[0]\n",
    "        # print(row)\n",
    "        combined_filename = '_'.join([row['Model'], row['Data Size'], str(row['idx']), str(i)])\n",
    "\n",
    "        yelp_select_output_command = \\\n",
    "            yelp_select_output_template.format(device_id=device_id,\n",
    "                                               pos2neg_filename=pos2neg_filename,\n",
    "                                               neg2pos_filename=neg2pos_filename,\n",
    "                                               combined_filename=combined_filename)\n",
    "\n",
    "        yelp_evaluate_command = \\\n",
    "            yelp_evaluate_template.format(device_id=device_id,\n",
    "                                          combined_filename=combined_filename)\n",
    "        all_commands += (\n",
    "                        generate_prompt_commands.tolist() + \n",
    "                        prompted_gpt2_commands.tolist() + \n",
    "                        [yelp_select_output_command, yelp_evaluate_command])\n",
    "    \n",
    "    subdf = subdf[['Model', 'Data Size', 'idx']].drop_duplicates()\n",
    "    subdf['commands'] = '\\n'.join(all_commands)\n",
    "    \n",
    "    return subdf\n",
    "\n",
    "df_experiment_commands = (df.groupby(['Model', 'Data Size', 'idx'], \n",
    "                                  as_index=False)\n",
    "                       .apply(create_commands, device_id=0)\n",
    "                       .reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ce8ffa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Size</th>\n",
       "      <th>idx</th>\n",
       "      <th>commands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model Data Size  idx  \\\n",
       "0    distilgpt2       all    0   \n",
       "1    distilgpt2       all    2   \n",
       "2    distilgpt2       all    3   \n",
       "3          gpt2       all    0   \n",
       "4          gpt2       all    2   \n",
       "5          gpt2       all    3   \n",
       "6    gpt2-large       all    0   \n",
       "7    gpt2-large       all    2   \n",
       "8    gpt2-large       all    3   \n",
       "9   gpt2-medium       all    0   \n",
       "10  gpt2-medium       all    2   \n",
       "11  gpt2-medium       all    3   \n",
       "12      gpt2-xl       all    0   \n",
       "13      gpt2-xl       all    2   \n",
       "14      gpt2-xl       all    3   \n",
       "\n",
       "                                             commands  \n",
       "0   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "1   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "2   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "3   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "4   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "5   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "6   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "7   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "8   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "9   CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "10  CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "11  CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "12  CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "13  CUDA_VISIBLE_DEVICES=0 python generate_prompts...  \n",
       "14  CUDA_VISIBLE_DEVICES=0 python generate_prompts...  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_experiment_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1c556266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=neg2pos --ckpt_path_short=2022-05-07/19-39-20/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_5.out\n",
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=pos2neg --ckpt_path_short=2022-05-07/19-37-29/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_5.out\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_5.out --src_path=./data/yelp/test.clean.0 --save_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_5.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_5.out --src_path=./data/yelp/test.clean.1 --save_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_5.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python yelp_output_selector.py select-and-save --device_id=0 --pos2neg_hypos_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_5.jsonl --neg2pos_hypos_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_5.jsonl --save_path=./outputs/selected/distilgpt2_all_2_5.jsonl\n",
      "python yelp_evaluator.py evaluate-and-save --device_id=0 --hypos_path=./outputs/selected/distilgpt2_all_2_5.jsonl --pos2neg_refs_path=./data/yelp/ref.clean.1 --neg2pos_refs_path=./data/yelp/ref.clean.0 --summary_save_path=./outputs/eval_summaries/distilgpt2_all_2_5.json --results_save_path=./outputs/eval_results/distilgpt2_all_2_5.csv\n",
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=neg2pos --ckpt_path_short=2022-05-07/19-39-20/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_6.out\n",
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=pos2neg --ckpt_path_short=2022-05-07/19-37-29/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_6.out\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_6.out --src_path=./data/yelp/test.clean.0 --save_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_6.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_6.out --src_path=./data/yelp/test.clean.1 --save_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_6.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python yelp_output_selector.py select-and-save --device_id=0 --pos2neg_hypos_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_6.jsonl --neg2pos_hypos_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_6.jsonl --save_path=./outputs/selected/distilgpt2_all_2_6.jsonl\n",
      "python yelp_evaluator.py evaluate-and-save --device_id=0 --hypos_path=./outputs/selected/distilgpt2_all_2_6.jsonl --pos2neg_refs_path=./data/yelp/ref.clean.1 --neg2pos_refs_path=./data/yelp/ref.clean.0 --summary_save_path=./outputs/eval_summaries/distilgpt2_all_2_6.json --results_save_path=./outputs/eval_results/distilgpt2_all_2_6.csv\n",
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=neg2pos --ckpt_path_short=2022-05-07/19-39-20/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_7.out\n",
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=pos2neg --ckpt_path_short=2022-05-07/19-37-29/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_7.out\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_7.out --src_path=./data/yelp/test.clean.0 --save_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_7.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_7.out --src_path=./data/yelp/test.clean.1 --save_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_7.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python yelp_output_selector.py select-and-save --device_id=0 --pos2neg_hypos_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_7.jsonl --neg2pos_hypos_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_7.jsonl --save_path=./outputs/selected/distilgpt2_all_2_7.jsonl\n",
      "python yelp_evaluator.py evaluate-and-save --device_id=0 --hypos_path=./outputs/selected/distilgpt2_all_2_7.jsonl --pos2neg_refs_path=./data/yelp/ref.clean.1 --neg2pos_refs_path=./data/yelp/ref.clean.0 --summary_save_path=./outputs/eval_summaries/distilgpt2_all_2_7.json --results_save_path=./outputs/eval_results/distilgpt2_all_2_7.csv\n",
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=neg2pos --ckpt_path_short=2022-05-07/19-39-20/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_8.out\n",
      "CUDA_VISIBLE_DEVICES=0 python generate_prompts.py --task=pos2neg --ckpt_path_short=2022-05-07/19-37-29/outputs/outputs.116.pth --device_id=0 --output_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_8.out\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_neg2pos_petuum-42_3_2_8.out --src_path=./data/yelp/test.clean.0 --save_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_8.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/distilgpt2_all_pos2neg_petuum-42_3_2_8.out --src_path=./data/yelp/test.clean.1 --save_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_8.jsonl --device_id=0 --model_name=distilgpt2\n",
      "python yelp_output_selector.py select-and-save --device_id=0 --pos2neg_hypos_path=./outputs/hypos/distilgpt2_all_pos2neg_petuum-42_3_2_8.jsonl --neg2pos_hypos_path=./outputs/hypos/distilgpt2_all_neg2pos_petuum-42_3_2_8.jsonl --save_path=./outputs/selected/distilgpt2_all_2_8.jsonl\n",
      "python yelp_evaluator.py evaluate-and-save --device_id=0 --hypos_path=./outputs/selected/distilgpt2_all_2_8.jsonl --pos2neg_refs_path=./data/yelp/ref.clean.1 --neg2pos_refs_path=./data/yelp/ref.clean.0 --summary_save_path=./outputs/eval_summaries/distilgpt2_all_2_8.json --results_save_path=./outputs/eval_results/distilgpt2_all_2_8.csv\n"
     ]
    }
   ],
   "source": [
    "print(df_experiment_commands.iloc[1].commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "927727f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_script(subdf, filename='all_data_evals_5_9'): \n",
    "    filename_idx = '_'.join([filename, str(subdf.iloc[0].idx)])\n",
    "    all_commands = '\\n\\n'.join(subdf.commands.tolist())\n",
    "    with open(os.path.join('../modules/scripts/', filename_idx + '.sh'), 'w') as fw: \n",
    "        fw.write(all_commands)\n",
    "    \n",
    "df_experiment_commands.groupby('idx', as_index=False).apply(write_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f869b0cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Size</th>\n",
       "      <th>idx</th>\n",
       "      <th>commands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>3</td>\n",
       "      <td>python yelp_evaluator.py evaluate-and-save --d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model Data Size  idx  \\\n",
       "0   gpt2-large       all    0   \n",
       "1   gpt2-large       all    2   \n",
       "2   gpt2-large       all    3   \n",
       "3  gpt2-medium       all    0   \n",
       "4  gpt2-medium       all    2   \n",
       "5  gpt2-medium       all    3   \n",
       "6      gpt2-xl       all    0   \n",
       "7      gpt2-xl       all    2   \n",
       "8      gpt2-xl       all    3   \n",
       "\n",
       "                                            commands  \n",
       "0  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "1  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "2  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "3  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "4  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "5  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "6  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "7  python yelp_evaluator.py evaluate-and-save --d...  \n",
       "8  python yelp_evaluator.py evaluate-and-save --d...  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_experiment_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0f5be21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(df_experiment_commands, filename='prompted_gpt2_evaluate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3e8fc",
   "metadata": {},
   "source": [
    "# Create commands for null prompt evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a02fea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = dict(Model='gpt2-xl', idx='3')\n",
    "    \n",
    "all_commands = []\n",
    "for i in range(10): \n",
    "    task_filenames = {}\n",
    "    task_commands = {}\n",
    "    for task in ['pos2neg', 'neg2pos']: \n",
    "        filename = '_'.join([row['Model'], task, 'null', row['idx'], str(i)])\n",
    "        task_filenames[task] = filename\n",
    "        \n",
    "        with open(f'../modules/outputs/prompts/{filename}.out', 'w') as fw: \n",
    "            for _ in range(500): \n",
    "                fw.write('\\n')\n",
    "            \n",
    "        prompted_gpt2_command = \\\n",
    "            prompted_gpt2_template.format(src_label=str(1 if task == 'pos2neg' else 0),\n",
    "                                          model_name=row['Model'],\n",
    "                                          device_id=row['idx'],\n",
    "                                          filename=filename)\n",
    "        task_commands[task] = prompted_gpt2_command\n",
    "        \n",
    "    combined_filename = '_'.join([row['Model'], 'null', str(row['idx']), str(i)])\n",
    "    yelp_select_output_command = \\\n",
    "        yelp_select_output_template.format(device_id=str(row['idx']),\n",
    "                                           pos2neg_filename=task_filenames['pos2neg'],\n",
    "                                           neg2pos_filename=task_filenames['neg2pos'],\n",
    "                                           combined_filename=combined_filename)\n",
    "    \n",
    "    yelp_evaluate_command = \\\n",
    "            yelp_evaluate_template.format(device_id=str(row['idx']),\n",
    "                                          combined_filename=combined_filename)\n",
    "    \n",
    "    all_commands += [task_commands['pos2neg'], \n",
    "                     task_commands['neg2pos'],\n",
    "                     yelp_select_output_command,\n",
    "                     yelp_evaluate_command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3cd75330",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('../modules/scripts/', 'null_evaluate' + '.sh'), 'w') as fw: \n",
    "    fw.write('\\n'.join(all_commands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652cb84",
   "metadata": {},
   "source": [
    "# Create commands for prompt transfer across model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4327a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./experiment-eval-configs-2.csv')\n",
    "df = df[(df['Data Size'] == 'all')]\n",
    "output_path = '/jupyter/prompt-generation/soft-Q-learning-for-text-generation/outputs/'\n",
    "df['checkpoint_exists'] = df['Checkpoint Path'].apply(lambda x: os.path.exists(os.path.join(output_path, x)))\n",
    "df['filename'] = (df['Model'] + '_' + \n",
    "                  df['Data Size'] + '_' + \n",
    "                  df['Task'] + '_' + \n",
    "                  df['Machine'] + '_' + \n",
    "                  df['Seed'].astype(str))\n",
    "file2drop = ['gpt2-xl_all_neg2pos_ec2-82_3',\n",
    "             'gpt2-xl_all_neg2pos_ec2-203_5']\n",
    "df = df[~df['filename'].isin(file2drop)]\n",
    "df['idx'] = df.groupby(['Model', 'Data Size', 'Task']).cumcount()\n",
    "df['filename'] = (df['Model'] + '_' + \n",
    "                      df['Data Size'] + '_' + \n",
    "                      df['Task'] + '_' + \n",
    "                      df['Machine'] + '_' + \n",
    "                      df['Seed'].astype(str) + '_' + \n",
    "                      df['idx'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0910108b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model', 'Data Size', 'Task', 'Machine', 'Seed', 'Checkpoint Path',\n",
       "       'Status', 'Note', 'checkpoint_exists', 'filename', 'idx'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "aa52545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df[['Model', 'idx']].drop_duplicates()\n",
    "df_cross = df.merge(df_sub, on='idx', suffixes=('', '_transfer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fb7e7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt_template = ('CUDA_VISIBLE_DEVICES={device_id} python generate_prompts.py --task={task} --ckpt_path_short={ckpt_path_short} '\n",
    "                            '--device_id=0 --output_path=./outputs/prompts/{filename}.out')\n",
    "\n",
    "prompted_gpt2_template = ('python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/{filename}.out '\n",
    "                          '--src_path=./data/yelp/test.clean.{src_label} --save_path=./outputs/hypos/{filename}.jsonl '\n",
    "                          '--device_id={device_id} --model_name={model_name}')\n",
    "\n",
    "yelp_select_output_template = ('python yelp_output_selector.py select-and-save --device_id={device_id} '\n",
    "                               '--pos2neg_hypos_path=./outputs/hypos/{pos2neg_filename}.jsonl '\n",
    "                               '--neg2pos_hypos_path=./outputs/hypos/{neg2pos_filename}.jsonl '\n",
    "                               '--save_path=./outputs/selected/{combined_filename}.jsonl')\n",
    "\n",
    "yelp_evaluate_template = ('python yelp_evaluator.py evaluate-and-save --device_id={device_id} '\n",
    "                          '--hypos_path=./outputs/selected/{combined_filename}.jsonl '\n",
    "                          '--pos2neg_refs_path=./data/yelp/ref.clean.1 '\n",
    "                          '--neg2pos_refs_path=./data/yelp/ref.clean.0 '\n",
    "                          '--summary_save_path=./outputs/eval_summaries/{combined_filename}.json '\n",
    "                          '--results_save_path=./outputs/eval_results/{combined_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c90c6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_commands(subdf, n_trials=5, device_id=None): \n",
    "    assert subdf.shape[0] == 2\n",
    "    # print(subdf.columns)\n",
    "    # print(subdf)\n",
    "    all_commands = []\n",
    "    for i in range(n_trials): \n",
    "        # transfer_filename = '_'.join([row['filename'], row['Model_transfer'], str(i)])\n",
    "        device_id = str(row['idx']) if device_id is None else str(device_id)\n",
    "        generate_prompt_commands = (subdf.apply(\n",
    "            lambda row: generate_prompt_template.format(task=row['Task'],\n",
    "                                                        ckpt_path_short=row['Checkpoint Path'],\n",
    "                                                        device_id=device_id,\n",
    "                                                        filename='_'.join([row['filename'], \n",
    "                                                                           row['Model_transfer'],\n",
    "                                                                           str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        prompted_gpt2_commands = (subdf.apply(\n",
    "            lambda row: prompted_gpt2_template.format(src_label=str(1 if row['Task'] == 'pos2neg' else 0),\n",
    "                                                      model_name=row['Model_transfer'],\n",
    "                                                      device_id=device_id,\n",
    "                                                      filename='_'.join([row['filename'], \n",
    "                                                                         row['Model_transfer'],\n",
    "                                                                         str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        pos2neg_row = subdf.query('Task == \"pos2neg\"').iloc[0]\n",
    "        pos2neg_filename = '_'.join([pos2neg_row.filename, \n",
    "                                     pos2neg_row['Model_transfer'],\n",
    "                                     str(i)])\n",
    "        neg2pos_row = subdf.query('Task == \"neg2pos\"').iloc[0]\n",
    "        neg2pos_filename = '_'.join([neg2pos_row.filename, \n",
    "                                     neg2pos_row['Model_transfer'],\n",
    "                                     str(i)])        \n",
    "        row = subdf.iloc[0]\n",
    "        # print(row)\n",
    "        combined_filename = '_'.join([row['Model'], \n",
    "                                      row['Data Size'], \n",
    "                                      str(row['idx']), \n",
    "                                      row['Model_transfer'],\n",
    "                                      str(i)])\n",
    "\n",
    "        yelp_select_output_command = \\\n",
    "            yelp_select_output_template.format(device_id=device_id,\n",
    "                                               pos2neg_filename=pos2neg_filename,\n",
    "                                               neg2pos_filename=neg2pos_filename,\n",
    "                                               combined_filename=combined_filename)\n",
    "\n",
    "        yelp_evaluate_command = \\\n",
    "            yelp_evaluate_template.format(device_id=device_id,\n",
    "                                          combined_filename=combined_filename)\n",
    "        all_commands += (\n",
    "#                         generate_prompt_commands.tolist() + \n",
    "#                         prompted_gpt2_commands.tolist() + \n",
    "                        [yelp_select_output_command, yelp_evaluate_command])\n",
    "    \n",
    "    subdf = subdf[['Model', 'Data Size', 'Model_transfer', 'idx']].drop_duplicates()\n",
    "    subdf['commands'] = '\\n'.join(all_commands)\n",
    "    \n",
    "    return subdf\n",
    "\n",
    "df_experiment_commands = (df_cross.groupby(['Model', 'Data Size', 'Model_transfer', 'idx'], \n",
    "                                  as_index=False)\n",
    "                       .apply(create_transfer_commands, device_id=0)\n",
    "                       .reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "344fcd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment_commands = df_experiment_commands[~(df_experiment_commands['Model'] ==\n",
    "                                                  df_experiment_commands['Model_transfer'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "9729dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_experiment_commands.iloc[0].commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "09e1e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_script(subdf, filename='all_data_evals', device_id=0): \n",
    "    filename_idx = '_'.join([filename, str(device_id)])\n",
    "    all_commands = '\\n\\n'.join(subdf.commands.tolist())\n",
    "    with open(os.path.join('../modules/scripts/', filename_idx + '.sh'), 'w') as fw: \n",
    "        fw.write(all_commands)\n",
    "    \n",
    "# df_experiment_commands.groupby('idx', as_index=False).apply(write_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b5f4d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(df_experiment_commands, filename='transfer_evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cf579fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commands_distil_large = df_experiment_commands.query('Model_transfer in [\"distilgpt2\", \"gpt2-large\"]')\n",
    "write_script(df_commands_distil_large, \n",
    "             filename='transfer_select_eval_distil_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a202c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(df_experiment_commands.query('Model_transfer in [\"gpt2\", \"gpt2-medium\"]'), \n",
    "             filename='transfer_select_eval_small_medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a8fb5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_script(df_experiment_commands.query('Model_transfer in [\"gpt2-xl\"]'), \n",
    "             filename='transfer_select_eval_xl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1716154",
   "metadata": {},
   "source": [
    "# Create commands for sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt_template = ('CUDA_VISIBLE_DEVICES={device_id} python generate_prompts.py --task={task} --ckpt_path_short={ckpt_path_short} '\n",
    "                            '--device_id=0 --output_path=./outputs/prompts/{filename}.out --sample_size=')\n",
    "\n",
    "prompted_gpt2_template = ('python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/{filename}.out '\n",
    "                          '--src_path=./data/yelp/test.clean.{src_label} --save_path=./outputs/hypos/{filename}.jsonl '\n",
    "                          '--device_id={device_id} --model_name={model_name}')\n",
    "\n",
    "yelp_select_output_template = ('python yelp_output_selector.py select-and-save --device_id={device_id} '\n",
    "                               '--pos2neg_hypos_path=./outputs/hypos/{pos2neg_filename}.jsonl '\n",
    "                               '--neg2pos_hypos_path=./outputs/hypos/{neg2pos_filename}.jsonl '\n",
    "                               '--save_path=./outputs/selected/{combined_filename}.jsonl')\n",
    "\n",
    "yelp_evaluate_template = ('python yelp_evaluator.py evaluate-and-save --device_id={device_id} '\n",
    "                          '--hypos_path=./outputs/selected/{combined_filename}.jsonl '\n",
    "                          '--pos2neg_refs_path=./data/yelp/ref.clean.1 '\n",
    "                          '--neg2pos_refs_path=./data/yelp/ref.clean.0 '\n",
    "                          '--summary_save_path=./outputs/eval_summaries/{combined_filename}.json '\n",
    "                          '--results_save_path=./outputs/eval_results/{combined_filename}.csv')\n",
    "\n",
    "def create_commands(subdf, n_trials=5, device_id=None): \n",
    "    assert subdf.shape[0] == 2\n",
    "    # print(subdf.columns)\n",
    "    # print(subdf)\n",
    "    all_commands = []\n",
    "    for i in range(5):\n",
    "        for sample_size in \n",
    "        generate_prompt_commands = (subdf.apply(\n",
    "            lambda row: generate_prompt_template.format(task=row['Task'],\n",
    "                                                        ckpt_path_short=row['Checkpoint Path'],\n",
    "                                                        device_id=device_id,\n",
    "                                                        filename='_'.join([row['filename_idx'], str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        prompted_gpt2_commands = (subdf.apply(\n",
    "            lambda row: prompted_gpt2_template.format(src_label=str(1 if row['Task'] == 'pos2neg' else 0),\n",
    "                                                      model_name=row['Model'],\n",
    "                                                      device_id=device_id,\n",
    "                                                      filename='_'.join([row['filename_idx'], str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        pos2neg_filename = '_'.join([subdf.query('Task == \"pos2neg\"').iloc[0].filename_idx, str(i)])\n",
    "        neg2pos_filename = '_'.join([subdf.query('Task == \"neg2pos\"').iloc[0].filename_idx, str(i)])\n",
    "        row = subdf.iloc[0]\n",
    "        # print(row)\n",
    "        combined_filename = '_'.join([row['Model'], row['Data Size'], str(row['idx']), str(i)])\n",
    "\n",
    "        yelp_select_output_command = \\\n",
    "            yelp_select_output_template.format(device_id=device_id,\n",
    "                                               pos2neg_filename=pos2neg_filename,\n",
    "                                               neg2pos_filename=neg2pos_filename,\n",
    "                                               combined_filename=combined_filename)\n",
    "\n",
    "        yelp_evaluate_command = \\\n",
    "            yelp_evaluate_template.format(device_id=device_id,\n",
    "                                          combined_filename=combined_filename)\n",
    "        all_commands += (\n",
    "                        generate_prompt_commands.tolist() + \n",
    "                        prompted_gpt2_commands.tolist() + \n",
    "                        [yelp_select_output_command, yelp_evaluate_command])\n",
    "    \n",
    "    subdf = subdf[['Model', 'Data Size', 'idx']].drop_duplicates()\n",
    "    subdf['commands'] = '\\n'.join(all_commands)\n",
    "    \n",
    "    return subdf\n",
    "\n",
    "df_experiment_commands = (df.groupby(['Model', 'Data Size', 'idx'], \n",
    "                                  as_index=False)\n",
    "                       .apply(create_commands, device_id=0)\n",
    "                       .reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0ff37",
   "metadata": {},
   "source": [
    "# Fluent prompt evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3cb1b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt_template = ('CUDA_VISIBLE_DEVICES={device_id} python generate_prompts.py --task={task} --ckpt_path_short={ckpt_path_short} '\n",
    "                            '--device_id=0 --output_path=./outputs/prompts/{filename}.out')\n",
    "\n",
    "prompted_gpt2_template = ('python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/{filename}.out '\n",
    "                          '--src_path=./data/yelp/test.clean.{src_label} --save_path=./outputs/hypos/{filename}.jsonl '\n",
    "                          '--device_id={device_id} --model_name={model_name}')\n",
    "\n",
    "yelp_select_output_template = ('python yelp_output_selector.py select-and-save --device_id={device_id} '\n",
    "                               '--pos2neg_hypos_path=./outputs/hypos/{pos2neg_filename}.jsonl '\n",
    "                               '--neg2pos_hypos_path=./outputs/hypos/{neg2pos_filename}.jsonl '\n",
    "                               '--save_path=./outputs/selected/{combined_filename}.jsonl')\n",
    "\n",
    "yelp_evaluate_template = ('python yelp_evaluator.py evaluate-and-save --device_id={device_id} '\n",
    "                          '--hypos_path=./outputs/selected/{combined_filename}.jsonl '\n",
    "                          '--pos2neg_refs_path=./data/yelp/ref.clean.1 '\n",
    "                          '--neg2pos_refs_path=./data/yelp/ref.clean.0 '\n",
    "                          '--summary_save_path=./outputs/eval_summaries/{combined_filename}.json '\n",
    "                          '--results_save_path=./outputs/eval_results/{combined_filename}.csv')\n",
    "\n",
    "def create_fluent_commands(subdf, n_trials=5, device_id=None): \n",
    "    assert subdf.shape[0] == 2\n",
    "    # print(subdf.columns)\n",
    "    # print(subdf)\n",
    "    all_commands = []\n",
    "    for i in range(5):\n",
    "        generate_prompt_commands = (subdf.apply(\n",
    "            lambda row: generate_prompt_template.format(task=row['Task'],\n",
    "                                                        ckpt_path_short=row['Checkpoint Path'],\n",
    "                                                        device_id=device_id,\n",
    "                                                        filename='_'.join([row['filename_idx'], \n",
    "                                                                           'fluent',\n",
    "                                                                           str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        prompted_gpt2_commands = (subdf.apply(\n",
    "            lambda row: prompted_gpt2_template.format(src_label=str(1 if row['Task'] == 'pos2neg' else 0),\n",
    "                                                      model_name=row['Model'],\n",
    "                                                      device_id=device_id,\n",
    "                                                      filename='_'.join([row['filename_idx'], \n",
    "                                                                         'fluent',\n",
    "                                                                         str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        pos2neg_filename = '_'.join([subdf.query('Task == \"pos2neg\"').iloc[0].filename_idx, \n",
    "                                     'fluent',\n",
    "                                     str(i)])\n",
    "        neg2pos_filename = '_'.join([subdf.query('Task == \"neg2pos\"').iloc[0].filename_idx, \n",
    "                                     'fluent',\n",
    "                                     str(i)])\n",
    "        row = subdf.iloc[0]\n",
    "        # print(row)\n",
    "        combined_filename = '_'.join([row['Model'], \n",
    "                                      row['Data Size'], \n",
    "                                      str(row['idx']), \n",
    "                                      'fluent',\n",
    "                                      str(i)])\n",
    "\n",
    "        yelp_select_output_command = \\\n",
    "            yelp_select_output_template.format(device_id=device_id,\n",
    "                                               pos2neg_filename=pos2neg_filename,\n",
    "                                               neg2pos_filename=neg2pos_filename,\n",
    "                                               combined_filename=combined_filename)\n",
    "\n",
    "        yelp_evaluate_command = \\\n",
    "            yelp_evaluate_template.format(device_id=device_id,\n",
    "                                          combined_filename=combined_filename)\n",
    "        all_commands += (\n",
    "                        generate_prompt_commands.tolist() + \n",
    "                        prompted_gpt2_commands.tolist() + \n",
    "                        [yelp_select_output_command, yelp_evaluate_command])\n",
    "    \n",
    "    subdf = subdf[['Model', 'Data Size', 'idx']].drop_duplicates()\n",
    "    subdf['commands'] = '\\n'.join(all_commands)\n",
    "    \n",
    "    return subdf\n",
    "\n",
    "df_fluent = df.query('Note == \"fluent\"')\n",
    "df_experiment_commands = (df_fluent.groupby(['Model', 'Data Size', 'idx'], \n",
    "                                  as_index=False)\n",
    "                       .apply(create_fluent_commands, device_id=0)\n",
    "                       .reset_index(drop=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "633f414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Size</th>\n",
       "      <th>idx</th>\n",
       "      <th>commands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model Data Size  idx                                           commands\n",
       "0  gpt2-xl       all    0  CUDA_VISIBLE_DEVICES=0 python generate_prompts...\n",
       "1  gpt2-xl       all    2  CUDA_VISIBLE_DEVICES=0 python generate_prompts..."
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_experiment_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a717c8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Data Size</th>\n",
       "      <th>idx</th>\n",
       "      <th>commands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>CUDA_VISIBLE_DEVICES=0 python generate_prompts...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model Data Size  idx                                           commands\n",
       "0  gpt2-xl       all    0  CUDA_VISIBLE_DEVICES=0 python generate_prompts...\n",
       "1  gpt2-xl       all    2  CUDA_VISIBLE_DEVICES=0 python generate_prompts..."
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_experiment_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "67dd3941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_script(subdf, filename='all_data_evals', device_id=0): \n",
    "    filename_idx = '_'.join([filename, str(subdf.iloc[0].idx)])\n",
    "    all_commands = '\\n\\n'.join(subdf.commands.tolist())\n",
    "    with open(os.path.join('../modules/scripts/', filename_idx + '.sh'), 'w') as fw: \n",
    "        fw.write(all_commands)\n",
    "        \n",
    "df_experiment_commands.groupby('idx', as_index=False).apply(write_script, filename='fluent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d76d57",
   "metadata": {},
   "source": [
    "# Manual prompt and augmented zero-shot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7c074b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_prompted_gpt2_template = ('python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/{filename}.out '\n",
    "                          '--src_path=./data/yelp/test.clean.{src_label} --save_path=./outputs/hypos/{filename}.jsonl '\n",
    "                          \"--device_id={device_id} --model_name={model_name} --tst_template='{tst_template}'\")\n",
    "\n",
    "yelp_select_output_template = ('python yelp_output_selector.py select-and-save --device_id={device_id} '\n",
    "                               '--pos2neg_hypos_path=./outputs/hypos/{pos2neg_filename}.jsonl '\n",
    "                               '--neg2pos_hypos_path=./outputs/hypos/{neg2pos_filename}.jsonl '\n",
    "                               '--save_path=./outputs/selected/{combined_filename}.jsonl')\n",
    "\n",
    "yelp_evaluate_template = ('python yelp_evaluator.py evaluate-and-save --device_id={device_id} '\n",
    "                          '--hypos_path=./outputs/selected/{combined_filename}.jsonl '\n",
    "                          '--pos2neg_refs_path=./data/yelp/ref.clean.1 '\n",
    "                          '--neg2pos_refs_path=./data/yelp/ref.clean.0 '\n",
    "                          '--summary_save_path=./outputs/eval_summaries/{combined_filename}.json '\n",
    "                          '--results_save_path=./outputs/eval_results/{combined_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "fb9fd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_zeroshot_block = 'Here is some text: \"When the doctor asked Linda to take the medicine, he smiled and gave her a lollipop.\". Here is a rewrite of the text, which is more scary. \"When the doctor told Linda to take the medicine, there had been a malicious gleam in her eye that Linda didn’t like at all.\" Here is some text: \"they asked loudly, over the sound of the train.\". Here is a rewrite of the text, which is more intense. \"they yelled aggressively, over the clanging of the train.\" Here is some text: \"When Mohammed left the theatre, it was already dark out\". Here is a rewrite of the text, which is more about the movie itself. \"The movie was longer than Mohammed had expected, and despite the excellent ratings he was a bit disappointed when he left the theatre.\" Here is some text: \"next to the path\". Here is a rewrite of the text, which is about France. \"next to la Siene\" Here is some text: \"The man stood outside the grocery store, ringing the bell.\". Here is a rewrite of the text, which is about clowns. \"The man stood outside the circus, holding a bunch of balloons.\" Here is some text: \"the bell ringing\". Here is a rewrite of the text, which is more flowery. \"the peales of the jangling bell\" Here is some text: \"against the tree\". Here is a rewrite of the text, which is include the word \"snow\". \"against the snow-covered bark of the tree\" '\n",
    "\n",
    "tst_templates = {'manual-1': dict(pos2neg='Here is some text: \"{sentence_1}\". Here is a rewrite of the text, which is more negative: \"',\n",
    "                                  neg2pos='Here is some text: \"{sentence_1}\". Here is a rewrite of the text, which is more positive: \"'),\n",
    "                 'manual-2': dict(pos2neg='Change the following sentence from positive sentiment to negative sentiment but keep its semantics. \"{sentence_1}\" \"',\n",
    "                                  neg2pos='Change the following sentence from negative sentiment to positive sentiment but keep its semantics. \"{sentence_1}\" \"'),\n",
    "                 'manual-3': dict(pos2neg='\"{sentence_1}\". Rewrite the sentence to be sadder but have the same meaning. \"',\n",
    "                                  neg2pos='\"{sentence_1}\". Rewrite the sentence to be happier but have the same meaning. \"'),\n",
    "                 'augmented-zeroshot': dict(pos2neg=augmented_zeroshot_block + 'Here is some text: \"{sentence_1}\". Here is a rewrite of the text, which is more negative. \"',\n",
    "                                            neg2pos=augmented_zeroshot_block + 'Here is some text: \"{sentence_1}\". Here is a rewrite of the text, which is more positive. \"')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6df65cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_commands = {}\n",
    "model_names = ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e615db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0\n",
    "for model_name in model_names: \n",
    "    all_commands = []\n",
    "    row = dict(Model=model_name, idx='3')\n",
    "    for i in range(5, 10): \n",
    "        for name, task_templates in tst_templates.items(): \n",
    "            task_filenames = {}\n",
    "            task_commands = {}\n",
    "            for task, template in task_templates.items(): \n",
    "                filename = '_'.join([row['Model'], task, name, str(row['idx']), str(i)])\n",
    "                with open(f'../modules/outputs/prompts/{filename}.out', 'w') as fw: \n",
    "                    for _ in range(500): \n",
    "                        fw.write('\\n')\n",
    "                task_filenames[task] = filename\n",
    "\n",
    "                prompted_gpt2_command = \\\n",
    "                    template_prompted_gpt2_template.format(src_label=str(1 if task == 'pos2neg' else 0),\n",
    "                                                  model_name=row['Model'],\n",
    "                                                  device_id=device_id,\n",
    "                                                  filename=filename,\n",
    "                                                    tst_template=template)\n",
    "\n",
    "                task_commands[task] = prompted_gpt2_command\n",
    "\n",
    "            combined_filename = '_'.join([row['Model'], name, str(row['idx']), str(i)])\n",
    "            yelp_select_output_command = \\\n",
    "                yelp_select_output_template.format(device_id=device_id,\n",
    "                                                   pos2neg_filename=task_filenames['pos2neg'],\n",
    "                                                   neg2pos_filename=task_filenames['neg2pos'],\n",
    "                                                   combined_filename=combined_filename)\n",
    "\n",
    "            yelp_evaluate_command = \\\n",
    "                    yelp_evaluate_template.format(device_id=device_id,\n",
    "                                                  combined_filename=combined_filename)\n",
    "\n",
    "            all_commands += [task_commands['pos2neg'], \n",
    "                             task_commands['neg2pos'],\n",
    "                             yelp_select_output_command,\n",
    "                             yelp_evaluate_command]\n",
    "    model_commands[model_name] = all_commands\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "71b7185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name, commands in model_commands.items(): \n",
    "#     with open(os.path.join('../modules/scripts/', 'manual_evaluate_' + model_name + '.sh'), 'w') as fw: \n",
    "#         fw.write('\\n'.join(commands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "0a240b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = model_commands['gpt2-xl']\n",
    "with open(os.path.join('../modules/scripts/', 'manual_evaluate_' + 'gpt2-xl_5_9_0' + '.sh'), 'w') as fw: \n",
    "    fw.write('\\n'.join(commands[:len(commands)//2]))\n",
    "with open(os.path.join('../modules/scripts/', 'manual_evaluate_' + 'gpt2-xl_5_9_1' + '.sh'), 'w') as fw: \n",
    "    fw.write('\\n'.join(commands[len(commands)//2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "b510ad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/gpt2-xl_pos2neg_manual-1_3_0.out --src_path=./data/yelp/test.clean.1 --save_path=./outputs/hypos/gpt2-xl_pos2neg_manual-1_3_0.jsonl --device_id=0 --model_name=gpt2-xl --tst_template='Here is some text: \"{sentence_1}\". Here is a rewrite of the text, which is more negative: \"'\n"
     ]
    }
   ],
   "source": [
    "print(all_commands[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6415f",
   "metadata": {},
   "source": [
    "# Random Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "b5195a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "generate_random_prompt_template = ('CUDA_VISIBLE_DEVICES={device_id} python generate_prompts.py --task={task} --ckpt_path_short=\"\" '\n",
    "                            '--device_id=0 --output_path=./outputs/prompts/{filename}.out')\n",
    "\n",
    "prompted_gpt2_template = ('python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/{filename}.out '\n",
    "                          '--src_path=./data/yelp/test.clean.{src_label} --save_path=./outputs/hypos/{filename}.jsonl '\n",
    "                          '--device_id={device_id} --model_name={model_name}')\n",
    "\n",
    "yelp_select_output_template = ('python yelp_output_selector.py select-and-save --device_id={device_id} '\n",
    "                               '--pos2neg_hypos_path=./outputs/hypos/{pos2neg_filename}.jsonl '\n",
    "                               '--neg2pos_hypos_path=./outputs/hypos/{neg2pos_filename}.jsonl '\n",
    "                               '--save_path=./outputs/selected/{combined_filename}.jsonl')\n",
    "\n",
    "yelp_evaluate_template = ('python yelp_evaluator.py evaluate-and-save --device_id={device_id} '\n",
    "                          '--hypos_path=./outputs/selected/{combined_filename}.jsonl '\n",
    "                          '--pos2neg_refs_path=./data/yelp/ref.clean.1 '\n",
    "                          '--neg2pos_refs_path=./data/yelp/ref.clean.0 '\n",
    "                          '--summary_save_path=./outputs/eval_summaries/{combined_filename}.json '\n",
    "                          '--results_save_path=./outputs/eval_results/{combined_filename}.csv')\n",
    "\n",
    "def create_random_commands(subdf, n_trials=5, device_id=None): \n",
    "    assert subdf.shape[0] == 2\n",
    "    # print(subdf.columns)\n",
    "    # print(subdf)\n",
    "    all_commands = []\n",
    "    for i in range(n_trials):\n",
    "        generate_prompt_commands = (subdf.apply(\n",
    "            lambda row: generate_random_prompt_template.format(task=row['Task'],\n",
    "                                                        device_id=device_id,\n",
    "                                                        filename='_'.join([row['Model'], \n",
    "                                                                           row['Task'], \n",
    "                                                                           'random', \n",
    "                                                                           str(row['idx']), \n",
    "                                                                           str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        prompted_gpt2_commands = (subdf.apply(\n",
    "            lambda row: prompted_gpt2_template.format(src_label=str(1 if row['Task'] == 'pos2neg' else 0),\n",
    "                                                      model_name=row['Model'],\n",
    "                                                      device_id=device_id,\n",
    "                                                      filename='_'.join([row['Model'], \n",
    "                                                                           row['Task'], \n",
    "                                                                           'random', \n",
    "                                                                           str(row['idx']), \n",
    "                                                                           str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        pos2neg_row = subdf.query('Task == \"pos2neg\"').iloc[0]\n",
    "        pos2neg_filename = '_'.join([pos2neg_row['Model'], \n",
    "                                     pos2neg_row['Task'], \n",
    "                                       'random', \n",
    "                                       str(pos2neg_row['idx']), \n",
    "                                       str(i)])\n",
    "        neg2pos_row = subdf.query('Task == \"neg2pos\"').iloc[0]\n",
    "        neg2pos_filename = '_'.join([neg2pos_row['Model'], \n",
    "                                     neg2pos_row['Task'], \n",
    "                                       'random', \n",
    "                                       str(neg2pos_row['idx']), \n",
    "                                       str(i)])\n",
    "        \n",
    "        for fname in [pos2neg_filename, neg2pos_filename]: \n",
    "            with open(f'../modules/outputs/prompts/{fname}.out', 'w') as fw: \n",
    "                for _ in range(500): \n",
    "                    random_tokens = np.random.choice(tokenizer.vocab_size, 5)\n",
    "                    prompt = tokenizer.decode(random_tokens)\n",
    "                    fw.write(prompt + '\\n')\n",
    "            \n",
    "        row = subdf.iloc[0]\n",
    "        # print(row)\n",
    "        combined_filename = '_'.join([row['Model'], \n",
    "                                      'random', \n",
    "                                      str(row['idx']), \n",
    "                                      str(i)])\n",
    "\n",
    "        yelp_select_output_command = \\\n",
    "            yelp_select_output_template.format(device_id=device_id,\n",
    "                                               pos2neg_filename=pos2neg_filename,\n",
    "                                               neg2pos_filename=neg2pos_filename,\n",
    "                                               combined_filename=combined_filename)\n",
    "\n",
    "        yelp_evaluate_command = \\\n",
    "            yelp_evaluate_template.format(device_id=device_id,\n",
    "                                          combined_filename=combined_filename)\n",
    "        all_commands += ['\\n'.join(\n",
    "                        # generate_prompt_commands.tolist() + \n",
    "                        prompted_gpt2_commands.tolist() + \n",
    "                        [yelp_select_output_command, yelp_evaluate_command])]\n",
    "    print(len(all_commands))\n",
    "    subdf = subdf[['Model', 'idx']].drop_duplicates()\n",
    "    # subdf['commands'] = all_commands# '\\n'.join(all_commands)\n",
    "    \n",
    "    return all_commands\n",
    "\n",
    "# df_experiment_commands = (df.groupby(['Model', 'Data Size', 'idx'], \n",
    "#                                   as_index=False)\n",
    "#                        .apply(create_commands, device_id=0)\n",
    "#                        .reset_index(drop=True))\n",
    "    \n",
    "sub_df = pd.DataFrame({'Model': ['gpt2-xl'] * 2,\n",
    "                       'idx': [0] * 2,\n",
    "                       'Task': ['pos2neg', 'neg2pos']})\n",
    "\n",
    "all_commands = create_random_commands(sub_df, n_trials=10, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "73fd512a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "sub_df = pd.DataFrame({'Model': ['distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large'] * 2,\n",
    "                       'idx': [0] * 8,\n",
    "                       'Task': ['pos2neg'] * 4 + ['neg2pos'] * 4})\n",
    "df_commands = sub_df.groupby(['Model']).apply(create_random_commands, n_trials=5, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "531c686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commands = df_commands.reset_index(name='all_commands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e0ce5c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_file(row): \n",
    "    for i, command in enumerate(row['all_commands']): \n",
    "        with open(f'../modules/scripts/random_eval_{row[\"Model\"]}_{str(i)}.sh', 'w') as fw: \n",
    "            fw.write(command)\n",
    "df_commands.apply(write_file, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1f6aa",
   "metadata": {},
   "source": [
    "# Beam Search Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd37e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt_template = ('CUDA_VISIBLE_DEVICES={device_id} python generate_prompts.py --task={task} --ckpt_path_short={ckpt_path_short} '\n",
    "                            '--device_id=0 --output_path=./outputs/prompts/{filename}.out')\n",
    "\n",
    "prompted_gpt2_template = ('python prompted_gpt2.py sample-and-save --prompt_path=./outputs/prompts/{filename}.out '\n",
    "                          '--src_path=./data/yelp/test.clean.{src_label} --save_path=./outputs/hypos/{filename}.jsonl '\n",
    "                          '--device_id={device_id} --model_name={model_name} '\n",
    "                          '--num_beams=10 --num_return_sequences=3')\n",
    "\n",
    "yelp_select_output_template = ('python yelp_output_selector.py select-and-save --device_id={device_id} '\n",
    "                               '--pos2neg_hypos_path=./outputs/hypos/{pos2neg_filename}.jsonl '\n",
    "                               '--neg2pos_hypos_path=./outputs/hypos/{neg2pos_filename}.jsonl '\n",
    "                               '--save_path=./outputs/selected/{combined_filename}.jsonl')\n",
    "\n",
    "yelp_evaluate_template = ('python yelp_evaluator.py evaluate-and-save --device_id={device_id} '\n",
    "                          '--hypos_path=./outputs/selected/{combined_filename}.jsonl '\n",
    "                          '--pos2neg_refs_path=./data/yelp/ref.clean.1 '\n",
    "                          '--neg2pos_refs_path=./data/yelp/ref.clean.0 '\n",
    "                          '--summary_save_path=./outputs/eval_summaries/{combined_filename}.json '\n",
    "                          '--results_save_path=./outputs/eval_results/{combined_filename}.csv')\n",
    "\n",
    "def create_beam_commands(subdf, n_trials=5, device_id=None): \n",
    "    assert subdf.shape[0] == 2\n",
    "    # print(subdf.columns)\n",
    "    # print(subdf)\n",
    "    all_commands = []\n",
    "    for i in range(n_trials):\n",
    "        generate_prompt_commands = (subdf.apply(\n",
    "            lambda row: generate_prompt_template.format(task=row['Task'],\n",
    "                                                        device_id=device_id,\n",
    "                                                        ckpt_path_short=row['Checkpoint Path'],\n",
    "                                                        filename='_'.join([row['Model'], \n",
    "                                                                           row['Task'], \n",
    "                                                                           'beam', \n",
    "                                                                           str(row['idx']), \n",
    "                                                                           str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        prompted_gpt2_commands = (subdf.apply(\n",
    "            lambda row: prompted_gpt2_template.format(src_label=str(1 if row['Task'] == 'pos2neg' else 0),\n",
    "                                                      model_name=row['Model'],\n",
    "                                                      device_id=device_id,\n",
    "                                                      filename='_'.join([row['Model'], \n",
    "                                                                           row['Task'], \n",
    "                                                                           'beam', \n",
    "                                                                           str(row['idx']), \n",
    "                                                                           str(i)])),\n",
    "            axis=1))\n",
    "\n",
    "        pos2neg_row = subdf.query('Task == \"pos2neg\"').iloc[0]\n",
    "        pos2neg_filename = '_'.join([pos2neg_row['Model'], \n",
    "                                     pos2neg_row['Task'], \n",
    "                                       'beam', \n",
    "                                       str(pos2neg_row['idx']), \n",
    "                                       str(i)])\n",
    "        neg2pos_row = subdf.query('Task == \"neg2pos\"').iloc[0]\n",
    "        neg2pos_filename = '_'.join([neg2pos_row['Model'], \n",
    "                                     neg2pos_row['Task'], \n",
    "                                       'beam', \n",
    "                                       str(neg2pos_row['idx']), \n",
    "                                       str(i)])\n",
    "        \n",
    "#         for fname in [pos2neg_filename, neg2pos_filename]: \n",
    "#             with open(f'../modules/outputs/prompts/{fname}.out', 'w') as fw: \n",
    "#                 for _ in range(500): \n",
    "#                     random_tokens = np.random.choice(tokenizer.vocab_size, 5)\n",
    "#                     prompt = tokenizer.decode(random_tokens)\n",
    "#                     fw.write(prompt + '\\n')\n",
    "            \n",
    "        row = subdf.iloc[0]\n",
    "        # print(row)\n",
    "        combined_filename = '_'.join([row['Model'], \n",
    "                                      'beam', \n",
    "                                      str(row['idx']), \n",
    "                                      str(i)])\n",
    "\n",
    "        yelp_select_output_command = \\\n",
    "            yelp_select_output_template.format(device_id=device_id,\n",
    "                                               pos2neg_filename=pos2neg_filename,\n",
    "                                               neg2pos_filename=neg2pos_filename,\n",
    "                                               combined_filename=combined_filename)\n",
    "\n",
    "        yelp_evaluate_command = \\\n",
    "            yelp_evaluate_template.format(device_id=device_id,\n",
    "                                          combined_filename=combined_filename)\n",
    "        all_commands += ['\\n'.join(\n",
    "                        generate_prompt_commands.tolist() + \n",
    "                        prompted_gpt2_commands.tolist() + \n",
    "                        [yelp_select_output_command, yelp_evaluate_command])]\n",
    "    # print(len(all_commands))\n",
    "    subdf = subdf[['Model', 'idx']].drop_duplicates()\n",
    "    # subdf['commands'] = all_commands# '\\n'.join(all_commands)\n",
    "    \n",
    "    return all_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4da72a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commands = df.groupby(['Model', 'Note', 'idx']).apply(create_beam_commands, n_trials=5, device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48e3bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commands = df_commands.reset_index(name='commands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e0b0400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    None\n",
       "7    None\n",
       "8    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_file(row): \n",
    "    # for i, command in enumerate(row['commands']): \n",
    "    with open(f'../modules/scripts/beam_eval_{row[\"Model\"]}_{str(row[\"idx\"])}.sh', 'w') as fw: \n",
    "        fw.write('\\n\\n'.join(row['commands']))\n",
    "df_commands.query('Model == \"gpt2-xl\" and Note != \"fluent\"').apply(write_file, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84fb091a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Note</th>\n",
       "      <th>idx</th>\n",
       "      <th>commands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>fluent</td>\n",
       "      <td>0</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>fluent</td>\n",
       "      <td>2</td>\n",
       "      <td>[CUDA_VISIBLE_DEVICES=0 python generate_prompt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model    Note  idx  \\\n",
       "0    gpt2-large            0   \n",
       "1    gpt2-large            2   \n",
       "2    gpt2-large            3   \n",
       "3   gpt2-medium            0   \n",
       "4   gpt2-medium            2   \n",
       "5   gpt2-medium            3   \n",
       "6       gpt2-xl            0   \n",
       "7       gpt2-xl            2   \n",
       "8       gpt2-xl            3   \n",
       "9       gpt2-xl  fluent    0   \n",
       "10      gpt2-xl  fluent    2   \n",
       "\n",
       "                                             commands  \n",
       "0   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "1   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "2   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "3   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "4   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "5   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "6   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "7   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "8   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "9   [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  \n",
       "10  [CUDA_VISIBLE_DEVICES=0 python generate_prompt...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728b138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql-203",
   "language": "python",
   "name": "sql-203"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
