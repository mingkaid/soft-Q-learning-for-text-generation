{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "velvet-buying",
   "metadata": {},
   "source": [
    "# Train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "collected-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [('./train.source.positive', './train.target.positive'),]\n",
    "for source_file, target_file in file_names: \n",
    "    \n",
    "    rep = 10000\n",
    "    control_lines = ['LABEL_1']\n",
    "    with open(source_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in control_lines: \n",
    "#                 input_ids = tokenizer(line, add_special_tokens=False)['input_ids']\n",
    "#                 subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#                 subword_str = ' '.join(subwords)\n",
    "                subword_str = line\n",
    "                fw.write(subword_str + '\\n')\n",
    "\n",
    "    target_lines = ['issues']\n",
    "    prompt_length = 5\n",
    "    with open(target_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in target_lines: \n",
    "                fw.write(' '.join([line] * prompt_length) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "white-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [('./train.source.negative', './train.target.negative'),]\n",
    "for source_file, target_file in file_names: \n",
    "    \n",
    "    rep = 10000\n",
    "    control_lines = ['LABEL_0']\n",
    "    with open(source_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in control_lines: \n",
    "#                 input_ids = tokenizer(line, add_special_tokens=False)['input_ids']\n",
    "#                 subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#                 subword_str = ' '.join(subwords)\n",
    "                subword_str = line\n",
    "                fw.write(subword_str + '\\n')\n",
    "\n",
    "    target_lines = ['issues']\n",
    "    prompt_length = 5\n",
    "    with open(target_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in target_lines: \n",
    "                fw.write(' '.join([line] * prompt_length) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-underwear",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "physical-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "featured-substitute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176787it [00:00, 413482.64it/s]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "with open('/jupyter/prompt-generation/soft-Q-learning-for-text-generation/'\n",
    "          'data/yelp-gpt2-control-only/raw/sentiment.train.0') as f: \n",
    "    for line in tqdm(f): \n",
    "        sentence = line.strip()\n",
    "        all_words += sentence.split()\n",
    "words = list(set(all_words))\n",
    "with open('/jupyter/prompt-generation/soft-Q-learning-for-text-generation/'\n",
    "          'data/yelp-gpt2-words/vocab.target.negative', 'w') as fw: \n",
    "    for word in words: \n",
    "        fw.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structured-piece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267314it [00:00, 524077.24it/s]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "with open('/jupyter/prompt-generation/soft-Q-learning-for-text-generation/'\n",
    "          'data/yelp-gpt2-control-only/raw/sentiment.train.1') as f: \n",
    "    for line in tqdm(f): \n",
    "        sentence = line.strip()\n",
    "        all_words += sentence.split()\n",
    "words = list(set(all_words))\n",
    "with open('/jupyter/prompt-generation/soft-Q-learning-for-text-generation/'\n",
    "          'data/yelp-gpt2-words/vocab.target.positive', 'w') as fw: \n",
    "    for word in words: \n",
    "        fw.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-ozone",
   "metadata": {},
   "source": [
    "# Dev files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "constitutional-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [('./dev.source.negative', './dev.target.negative'),]\n",
    "for source_file, target_file in file_names: \n",
    "    \n",
    "    rep = 10\n",
    "    control_lines = ['LABEL_0']\n",
    "    with open(source_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in control_lines: \n",
    "#                 input_ids = tokenizer(line, add_special_tokens=False)['input_ids']\n",
    "#                 subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#                 subword_str = ' '.join(subwords)\n",
    "                subword_str = line\n",
    "                fw.write(subword_str + '\\n')\n",
    "\n",
    "    target_lines = ['issues']\n",
    "    prompt_length = 5\n",
    "    with open(target_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in target_lines: \n",
    "                fw.write(' '.join([line] * prompt_length) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "earned-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [('./dev.source.positive', './dev.target.positive'),]\n",
    "for source_file, target_file in file_names: \n",
    "    \n",
    "    rep = 10\n",
    "    control_lines = ['LABEL_1']\n",
    "    with open(source_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in control_lines: \n",
    "#                 input_ids = tokenizer(line, add_special_tokens=False)['input_ids']\n",
    "#                 subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#                 subword_str = ' '.join(subwords)\n",
    "                subword_str = line\n",
    "                fw.write(subword_str + '\\n')\n",
    "\n",
    "    target_lines = ['issues']\n",
    "    prompt_length = 5\n",
    "    with open(target_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in target_lines: \n",
    "                fw.write(' '.join([line] * prompt_length) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2b645",
   "metadata": {},
   "source": [
    "# Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb0228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [('./test.source.negative', './test.target.negative'),]\n",
    "for source_file, target_file in file_names: \n",
    "    \n",
    "    rep = 500\n",
    "    control_lines = ['LABEL_0']\n",
    "    with open(source_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in control_lines: \n",
    "#                 input_ids = tokenizer(line, add_special_tokens=False)['input_ids']\n",
    "#                 subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#                 subword_str = ' '.join(subwords)\n",
    "                subword_str = line\n",
    "                fw.write(subword_str + '\\n')\n",
    "\n",
    "    target_lines = ['issues']\n",
    "    prompt_length = 5\n",
    "    with open(target_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in target_lines: \n",
    "                fw.write(' '.join([line] * prompt_length) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8474bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [('./test.source.positive', './test.target.positive'),]\n",
    "for source_file, target_file in file_names: \n",
    "    \n",
    "    rep = 500\n",
    "    control_lines = ['LABEL_1']\n",
    "    with open(source_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in control_lines: \n",
    "#                 input_ids = tokenizer(line, add_special_tokens=False)['input_ids']\n",
    "#                 subwords = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#                 subword_str = ' '.join(subwords)\n",
    "                subword_str = line\n",
    "                fw.write(subword_str + '\\n')\n",
    "\n",
    "    target_lines = ['issues']\n",
    "    prompt_length = 5\n",
    "    with open(target_file, 'w') as fw: \n",
    "        for _ in range(rep): \n",
    "            for line in target_lines: \n",
    "                fw.write(' '.join([line] * prompt_length) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql-203",
   "language": "python",
   "name": "sql-203"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
